{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the NGFF Workshop!","text":"<p>This workshop covers several key steps for working with OME-Zarr datasets,  including converting, inspecting, validating, opening, saving,  and visualizing OME-Zarr data.</p> <p>The sections given below are intended to guide you through working with  OME-Zarr datasets using a variety of tools and libraries.  Each section is designed to cover a specific task and introduces some of the existing tools to address that particular task.</p> <p>It is important to note that this workshop covers OME-Zarr version 0.4. With transition to the version 0.5, many changes have been implemented in the  specifications. These changes are, however, not covered or discussed here.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":""},{"location":"#data","title":"Data","text":"<p>The workshop will use several example image datasets.  Please download the zip file  and extract the datasets to a convenient directory.</p>"},{"location":"#installation-of-the-tools","title":"Installation of the tools","text":"<p>The easiest way to install the required tools is via mamba.</p> <p>Installing Mamba</p> <p>If you do not already have mamba, you can acquire it via Miniforge. Follow the instructions here to set  up Miniforge on your system.</p> <ul> <li>Unix-based systems (Linux/macOS): Installing Miniforge will make the mamba command available in your terminal.</li> <li>Windows: Miniforge installation will set up a new command prompt called Miniforge Prompt. You can find it by typing \"Miniforge\" in the Windows search bar. The mamba command will be available in this prompt.</li> </ul> <p>Setting Up the Workshop Environment</p> <p>Once mamba is installed, run the following commands to clean up any existing package cache and create the environment: <pre><code>mamba clean --all\nmamba create -n ngff_workshop -c conda-forge -c euro-bioimaging eubi-bridge napari-ome-zarr pyqt=5.*\n</code></pre> \u26a0\ufe0f Important: older installations of conda may be too slow to create the environment, especially with weak internet connection. So it is strongly recommended to install miniforge.</p>"},{"location":"#fiji-installation","title":"Fiji Installation","text":"<p>Ensure that an up-to-date version of Fiji  is installed on your system. </p>"},{"location":"#sections","title":"Sections","text":""},{"location":"#introduction","title":"Introduction","text":""},{"location":"#inspecting-and-validating-ome-zarr","title":"Inspecting and Validating OME-Zarr","text":""},{"location":"#converting-data-to-ome-zarr","title":"Converting Data to OME-Zarr","text":""},{"location":"#opening-ome-zarr","title":"Opening OME-Zarr","text":""},{"location":"#saving-ome-zarr","title":"Saving OME-Zarr","text":""},{"location":"conversion_overview/","title":"Overview","text":"<p>Convert collections of image data from various file formats to OME-Zarr.</p>"},{"location":"conversion_overview/#modules","title":"Modules","text":"<ul> <li>EuBI-Bridge</li> <li>NGFF Converter</li> <li>BatchConvert</li> </ul>"},{"location":"inspection_overview/","title":"Overview","text":"<p>Inspect and validate local (file system) and remote (S3 object store) OME-Zarr data</p>"},{"location":"inspection_overview/#modules","title":"Modules","text":"<ul> <li>ome-zarr-py</li> <li>OME-Zarr Validator</li> </ul>"},{"location":"ome_zarr_conversion_ngff-converter/","title":"NGFF-Converter","text":"<ul> <li>Open the NGFF converter; tool website <ul> <li>Drag and drop the image file</li> <li>A dialog will open:<ul> <li>Output format: OME-NGFF</li> <li>Output location: Choose some folder on your computer</li> <li>[ Apply ]</li> </ul> </li> <li>[ Run Jobs ]</li> </ul> </li> <li>Inspect the OME-NGFF output using your file browser</li> <li>Inspect the XML in the OME subfolder using your web browser</li> <li>Open the OME-NGFF images using a tool of your choice, e.g. Fiji N5</li> </ul>"},{"location":"ome_zarr_creation_BatchConvert/","title":"BatchConvert","text":"<p>Perform parallelised conversion of image data collections to OME-Zarr using BatchConvert</p> <p>Important note: BatchConvert is currently only supported on unix-based systems</p> <p>As input, use the <code>tiff_series</code> dataset.</p>"},{"location":"ome_zarr_creation_BatchConvert/#unary-conversion","title":"Unary conversion","text":"<p><pre><code>batchconvert omezarr \\\n/path/to/tiff_series \\\n/path/to/output_dir \n</code></pre> This command maps each input file in the <code>tiff_series</code> folder to a single OME-Zarr,  which is then transferred to the <code>output_dir</code> folder.</p> <p>Check the content of the <code>output_dir</code> folder: <pre><code>ls /path/to/output_dir\n</code></pre> Optional: Inspect the created OME-Zarr. </p>"},{"location":"ome_zarr_creation_BatchConvert/#aggregative-conversion","title":"Aggregative conversion","text":"<p><pre><code>batchconvert omezarr \\\n--merge_files \\\n/path/to/tiff_series \\\n/path/to/concatenated_output_dir\n</code></pre> This conversion mode assumes that the input files are part of the same image and thus will merge them along a specific axis during the conversion process. The <code>--merge_files</code> flag specifies the grouped conversion option.</p> <p>Check the content of the <code>output_dir</code> folder: <pre><code>ls /path/to/concatenated_output_dir\n</code></pre> Optional: Inspect the created OME-Zarr. </p>"},{"location":"ome_zarr_creation_BatchConvert/#aggregative-conversion-with-specific-chunking-downscaling-and-compression-parameters","title":"Aggregative conversion with specific chunking, downscaling and compression parameters","text":"<p><pre><code>batchconvert omezarr \\\n--merge_files \\\n--compression_zarr zlib \\\n-ms 32 \\\n-cx 32 \\\n-cy 32 \\\n-cz 6 \\\n/path/to/tiff_series \\\n/path/to/concatenated_rechunked_output_dir\n</code></pre> Here we do not only concatenate images, but we create a resolution pyramid and specify chunk sizes in x, y and z dimensions.</p> <p>Check the content of the <code>zarr_series_concatenated_rechunked</code> folder: <pre><code>ls /path/to/concatenated_rechunked_output_dir\n</code></pre> Optional: Inspect the created OME-Zarr. Compare it to the one created earlier.</p>"},{"location":"ome_zarr_creation_EuBI-Bridge/","title":"EuBI-Bridge","text":"<p>Perform parallelised conversion of image data collections to OME-Zarr using EuBI-Bridge</p> <p>On the terminal browse into the directory named <code>example_images</code>: </p> <p><code>cd /path/to/data/example_images</code></p> <p>Activate the conda environment <code>ngff_workshop</code>:</p> <pre><code>conda activate ngff_workshop\n</code></pre>"},{"location":"ome_zarr_creation_EuBI-Bridge/#configure-the-memory-limit","title":"Configure the memory limit","text":"<pre><code>eubi configure_cluster --memory_limit 5GB\neubi show_config\n</code></pre>"},{"location":"ome_zarr_creation_EuBI-Bridge/#unary-conversion","title":"Unary Conversion","text":"<p>Given a dataset structured as follows:  </p> \ud83d\udcc2 multichannel_timeseries \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0001.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0002.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0003.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0004.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0001.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0002.tif \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0003.tif \u2514\u2500\u2500 \ud83d\udcc4 Channel2-T0004.tif <p>To convert each TIFF into a separate OME-Zarr container (unary conversion):  </p> <pre><code>eubi to_zarr multichannel_timeseries multichannel_timeseries_zarr\n</code></pre> <p>This produces:  </p> \ud83d\udcc2 multichannel_timeseries_zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0001.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0002.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0003.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T0004.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0001.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0002.zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel2-T0003.zarr \u2514\u2500\u2500 \ud83d\udcc4 Channel2-T0004.zarr"},{"location":"ome_zarr_creation_EuBI-Bridge/#aggregative-conversion-concatenation-along-dimensions","title":"Aggregative Conversion (Concatenation Along Dimensions)","text":"<p>To concatenate images along specific dimensions, EuBI-Bridge needs to be informed of file patterns that specify image dimensions. For this example, the file pattern for the channel dimension is <code>Channel</code>, which is followed by the channel index, and the file pattern for the time dimension is <code>T</code>, which is followed by the time index.</p> <p>For concatenation along the time dimension:</p> <p><pre><code>eubi to_zarr multichannel_timeseries multichannel_timeseries_concat-t_zarr --channel_tag Channel --time_tag T --concatenation_axes t\n</code></pre> Output:  </p> \ud83d\udcc2 multichannel_timeseries_time_concat-t_zarr \u251c\u2500\u2500 \ud83d\udcc4 Channel1-T_tset.zarr \u2514\u2500\u2500 \ud83d\udcc4 Channel2-T_tset.zarr <p>Important note: if the <code>--channel_tag</code> were not provided, the tool would not be aware of the multiple channels in the image and try to concatenate all images into a single one-channeled OME-Zarr. Therefore,  when an aggregative conversion is performed, all dimensions existing in the input files must be specified via their respective tags. </p> <p>For multidimensional concatenation (channel + time):</p> <pre><code>eubi to_zarr multichannel_timeseries multichannel_timeseries_concat-ct_zarr --channel_tag Channel --time_tag T --concatenation_axes ct\n</code></pre> <p>Note that both axes are specified with the argument <code>--concatenation_axes ct</code>.</p> <p>Output:  </p> \ud83d\udcc2 multichannel_timeseries_concat-ct_zarr \u2514\u2500\u2500 \ud83d\udcc4 Channel_cset-T_tset.zarr"},{"location":"ome_zarr_creation_EuBI-Bridge/#handling-nested-directories","title":"Handling Nested Directories","text":"<p>For datasets stored in nested directories such as:  </p> \ud83d\udcc2 multichannel_timeseries_nested   \u251c\u2500\u2500 \ud83d\udcc1 Channel1   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0001.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0002.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0003.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0004.tif   \u251c\u2500\u2500 \ud83d\udcc1 Channel2   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0001.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0002.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0003.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0004.tif <p>EuBI-Bridge automatically detects the nested structure. For multidimensional concatenation:  </p> <pre><code>eubi to_zarr multichannel_timeseries_nested multichannel_timeseries_nested_concat-ct_zarr --channel_tag Channel --time_tag T --concatenation_axes ct\n</code></pre> <p>Output:  </p> <pre><code>multichannel_timeseries_nested_concat-ct_zarr\n\u2514\u2500\u2500 Channel_cset-T_tset.zarr\n</code></pre>"},{"location":"ome_zarr_creation_EuBI-Bridge/#selective-data-conversion-using-wildcards","title":"Selective Data Conversion Using Wildcards","text":"<p>To process only specific files, wildcards can be used.  For example, to concatenate only lsm images from the <code>pff</code> directory:  </p> <pre><code>eubi to_zarr \"pff/*.lsm\" lsm_to_zarr\n</code></pre> <p>Note: When using wildcards, the input directory path must be enclosed in quotes as shown in the example above.  </p> <p>Output:  </p> \ud83d\udcc2 lsm_to_zarr    \u251c\u2500\u2500 \ud83d\udcc1 FtsZ2-1_GFP_KO2-1_no10G.zarr    \u2514\u2500\u2500 \ud83d\udcc1 FtsZ2-1_GFP_KO2-1_no16G.zarr"},{"location":"ome_zarr_creation_EuBI-Bridge/#handling-categorical-dimension-patterns","title":"Handling Categorical Dimension Patterns","text":"<p>For datasets where channel names are categorical such as in:</p> \ud83d\udcc2 blueredchannels_timeseries_nested   \u251c\u2500\u2500 \ud83d\udcc1 Blue   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0001.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0002.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0003.tif   \u2502\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc4 T0004.tif   \u2514\u2500\u2500 \ud83d\udcc1 Red   \u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0001.tif   \u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0002.tif   \u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc4 T0003.tif   \u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc4 T0004.tif <p>One can run the exact same command:</p> <pre><code>eubi to_zarr blueredchannels_timeseries_nested blueredchannels_timeseries_nested_concat-ct_zarr --channel_tag Blue,Red --time_tag T --concatenation_axes ct\n</code></pre> <p>Output:  </p> \ud83d\udcc2 blueredchannels_timeseries_nested_concat-ct_zarr \u2514\u2500\u2500 \ud83d\udcc4 BlueRed_cset-T_tset.zarr"},{"location":"ome_zarr_creation_EuBI-Bridge/#extraction-of-single-series-from-a-multi-series-dataset","title":"Extraction of Single Series from a Multi-series Dataset","text":"<pre><code>eubi to_zarr pff/17_03_18.lif lif_series_to_zarr --series 21 --no_distributed True\n</code></pre> <p>Output:  </p> \ud83d\udcc2 lif_series_to_zarr     \u2514\u2500\u2500 \ud83d\udcc1 17_03_18.lif-17_03_18_FtsZ2-2_no11.zarr"},{"location":"ome_zarr_inspection_ome-zarr-py/","title":"ome-zarr-py","text":"<p>Use the ome-zarr-py library for inspecting and downloading OME-Zarrs from s3</p> <p>Remote OME-Zarr data stored in a public s3 bucket can be inspected and downloaded using  the <code>ome-zarr-py</code> tool via terminal or Python code. </p> <p>Activate the conda environment <code>ngff_workshop</code>:</p> <pre><code>conda activate ngff_workshop\n</code></pre>"},{"location":"ome_zarr_inspection_ome-zarr-py/#terminal","title":"Terminal","text":""},{"location":"ome_zarr_inspection_ome-zarr-py/#inspect-remote-data","title":"Inspect remote data","text":"<p>Inspect 3 different remote datasets:</p> <pre><code>ome_zarr info https://uk1s3.embassy.ebi.ac.uk/EuBI/anna_steyer0/20160112_C.elegans_std_fullhead.zarr\n</code></pre> <pre><code>ome_zarr info https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\n</code></pre> <pre><code>ome_zarr info https://s3.embl.de/i2k-2020/platy-raw.ome.zarr\n</code></pre>"},{"location":"ome_zarr_inspection_ome-zarr-py/#download-remote-data","title":"Download remote data","text":"<p>Create a directory named <code>downloaded_omezarrs</code> in your home directory,  browse into it and download the dataset <code>6001240.zarr</code> from s3:</p> <pre><code>cd /path/to/data/downloaded_omezarrs\n</code></pre> <pre><code>ome_zarr download https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\n</code></pre> <p>Inspect the local dataset (optional): <pre><code>ome_zarr info /path/to/data/downloaded_omezarrs/6001240.zarr\n</code></pre></p>"},{"location":"ome_zarr_inspection_ome-zarr-py/#python","title":"Python","text":"<p>Access Python</p> <pre><code>python\n</code></pre>"},{"location":"ome_zarr_inspection_ome-zarr-py/#import-the-relevant-tools","title":"Import the relevant tools","text":"<pre><code>from ome_zarr import utils\n</code></pre>"},{"location":"ome_zarr_inspection_ome-zarr-py/#inspect-remote-data_1","title":"Inspect remote data","text":"<pre><code>list(utils.info(\"https://uk1s3.embassy.ebi.ac.uk/EuBI/anna_steyer0/20160112_C.elegans_std_fullhead.zarr\"))\n</code></pre>"},{"location":"ome_zarr_inspection_ome-zarr-py/#download-remote-data_1","title":"Download remote data","text":"<pre><code>utils.download(input_path = \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\",\n               output_dir = \"./downloaded_omezarrs\")\n</code></pre>"},{"location":"ome_zarr_open_java_n5-ij/","title":"n5-ij","text":"<p>Here we cover how to open OME-Zarr as a Fiji image.</p>"},{"location":"ome_zarr_open_java_n5-ij/#open-a-remote-ome-zarr-in-fiji","title":"Open a remote OME-Zarr in Fiji","text":"<p>Open the n5-ij in Fiji via: </p> <pre><code>[ File &gt; Import &gt; HDF5/N5/Zarr/OME-NGFF ... ]\n</code></pre> <p>On the window that opens, paste the following path in the uri space:</p> <pre><code>https://s3.embl.de/ome-zarr-course/data/commons/xyz_8bit_calibrated__fib_sem_crop.ome.zarr\n</code></pre> <p>Then click <code>Detect datasets</code> button as shown below: </p> <p></p> <p>The tool will display a multiscales schema with two datasets in the dialog box. Select one of the datasets as shown below and click OK:</p> <p></p> <p>This will open the dataset in Fiji as a normal Fiji image (see below). </p>"},{"location":"ome_zarr_open_java_n5-ij/#open-a-subset-of-a-remote-ome-zarr-in-fiji","title":"Open a Subset of a Remote OME-Zarr in Fiji","text":"<p>Follow the same steps above do select a dataset but instead of directly opening the dataset, click the crop button in the window before clicking OK as shown below: </p> <p></p> <p>In the window that open, select the indices of the subset as shown below:  </p> <p></p> <p>When you click OK, the specified subset of the image will be opened as shown below: </p> <p></p>"},{"location":"ome_zarr_open_java_n5-viewer/","title":"n5-viewer","text":"<p>Imagine that you want to open a dataset in Fiji, but it is too large to fit the RAM of your machine.</p>"},{"location":"ome_zarr_open_java_n5-viewer/#opening-a-remote-ome-zarr-in-bigdataviewer","title":"Opening a Remote OME-Zarr in BigDataViewer","text":"<p>Open the n5-viewer in Fiji via: </p> <p><code>[ Plugins &gt; BigDataViewer &gt; HDF5/N5/Zarr/OME-NGFF Viewer ]</code></p> <p>In the window that opens, paste the following path in the uri space:</p> <pre><code>https://s3.embl.de/i2k-2020/platy-raw.ome.zarr\n</code></pre> <p>Then click <code>Detect datasets</code> button as shown below: </p> <p></p> <p>The tool will display a multiscales schema with 9 datasets in the dialog box. In this case, one can either open the individual datasets or the entire pyramid.  To do the latter, click on the multiscale object and then click OK as shown below: </p> <p></p> <p>This will open the multiscales object in BDV as shown below: </p> <p></p>"},{"location":"ome_zarr_open_java_n5-viewer/#extraction-of-a-subset-of-the-ome-zarr-from-bigdataviewer","title":"Extraction of a Subset of the OME-Zarr From BigDataViewer","text":"<p>The dataset opened as shown above is a huge (terabyte-scale) image, which is not amenable to processing as a whole in Fiji. It is possible, however, to extract subsets of it to Fiji and continue with processing. To do so, follow the steps below:</p> <p>In the BDV window, open the cropping window via:  <code>[ Tools &gt; Extract to ImageJ ]</code> (also see below)  </p> <p>In the cropping window that opens, select the indices of the subset as shown below: </p> <p> </p> <p>Note that this step may require incremental rotation of the image and adjustment of the bounding box until the desired region of interest is obtained. It is also important to check the size of the cropped volume  at the top of the cropping window to make sure that it is not larger than the memory. Once you are fine with the settings, click OK. </p> <p>The output is a standard Fiji image as shown below: </p> <p></p> <p>Note that this image has been loaded into the RAM; as such, it can be processed like any other  Fiji image and saved to any desired file format. </p>"},{"location":"ome_zarr_open_mobie/","title":"MoBIE","text":"<ul> <li>Run Fiji with MoBIE</li> <li>Open OME-Zar with labels and label table in MoBIE:</li> <li><code>[ Plugins \u203a MoBIE \u203a Open \u203a Open OME ZARR... ]</code><ul> <li>Image URI: <code>https://s3.embl.de/i2k-2020/platy-raw.ome.zarr</code> (8 TB)</li> <li>Labels URI: <code>https://s3.embl.de/i2k-2020/platy-raw.ome.zarr/labels/cells</code> </li> <li>Labels Table URI: <code>https://raw.githubusercontent.com/mobie/platybrowser-project/refs/heads/main/data/1.0.1/tables/sbem-6dpf-1-whole-segmented-cells/default.tsv</code></li> <li>Note that the labels table is not integrated within the OME-Zarr, because a specification for this does not, despite hard work, yet exist. MoBIE supports a number of table formats and column names (see here).</li> </ul> </li> </ul>"},{"location":"ome_zarr_visualisation_napari/","title":"napari-ome-zarr","text":"<p>Here we cover different ways to open remote and local OME-Zarrs in Napari</p>"},{"location":"ome_zarr_visualisation_napari/#drag-and-drop","title":"Drag and drop","text":"<p>Open napari via terminal: <pre><code>napari\n</code></pre></p> <p>Now drag and drop one of the local OME-Zarrs (e.g., <code>6001240.zarr</code>) into <code>napari</code>.</p> <p>You will be prompted to choose a plugin. Choose <code>napari-ome-zarr</code>  and click <code>OK</code>.</p> <p>This is a convenient way to open local OME-Zarrs.</p>"},{"location":"ome_zarr_visualisation_napari/#use-command-line","title":"Use command line","text":"<p>This method can be used to open both local and remotely stored OME-Zarrs. For instance, use the following command to open an OME-Zarr from EBI's s3 bucket.</p> <pre><code>napari --plugin napari-ome-zarr https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\n</code></pre>"},{"location":"ome_zarr_visualisation_napari/#use-python-code","title":"Use Python code","text":"<p>Similarly to the command line option, local and remote OME-Zarrs can also be read directly via Python code.</p> <p>Approach 1: Open the full OME-Zarr from the top level url: <pre><code>import napari\n\nv = napari.Viewer()\nv.open(\"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\",\n       plugin = 'napari-ome-zarr'\n       )\nnapari.run()\n</code></pre> Note that this approach automates a lot of tasks for the user, discovering look-up tables, pixel scalings and units from the OME-Zarr metadata.</p> <p>Approach 2: Read arrays and open them individually: <pre><code>import napari\nimport zarr, dask.array as da\n\nurl = \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\"\ngr = zarr.open_group(url, mode = 'r')\n# Get the resolution layer 2 from the raw data as dask array.\narray2 = da.from_zarr(gr[2]) \n# Get the resolution layer 2 from the label data as dask array.\nlabel_array2 = da.from_zarr(gr.labels['0'][2]) \n# Open the viewer and add the dask arrays.\nv = napari.Viewer()\nv.add_image(array2, contrast_limits = (0, 2000), colormap = 'red')\nv.add_labels(label_array2)\nnapari.run()\n</code></pre></p> <p>Note that approach 2 does not read any metadata. You have to specify the metadata manually in the viewer's api.</p>"},{"location":"open_local_ome_zarr_zarr-python/","title":"zarr-python","text":"<p>Activate the conda environment <code>ngff_workshop</code>:</p> <pre><code>conda activate ngff_workshop\n</code></pre> <p>Browse into the example data directory and then access Python: </p> <pre><code>cd /path/to/example_omezarrs\npython\n</code></pre>"},{"location":"open_local_ome_zarr_zarr-python/#import-the-relevant-tools","title":"Import the relevant tools","text":"<pre><code>import zarr, os, pprint\nimport numpy as np\n</code></pre>"},{"location":"open_local_ome_zarr_zarr-python/#read-remote-local-and-remote-ome-zarrs","title":"Read remote local and remote OME-Zarrs","text":"<pre><code># path = \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\"\npath = \"./6001240.zarr\"\ndataset = zarr.open_group(path, mode = 'r')\n</code></pre>"},{"location":"open_local_ome_zarr_zarr-python/#inspect-the-group-level-metadata","title":"Inspect the group-level metadata","text":"<p>Print the data type <pre><code>print(f\"Type of the dataset: {type(dataset)}\")\n</code></pre></p> <p>Summarize group-level metadata: <pre><code>dataset.info\n</code></pre> Note the store type, the number of arrays and groups. \\ Note also the group named 'labels'.</p> <p>Print the full metadata: <pre><code>pprint.pprint(dict(dataset.attrs))\n</code></pre></p> <p>Get multiscales metadata: <pre><code>meta = dict(dataset.attrs['multiscales'][0])\n</code></pre></p> <p>Print the axis ordering and the units <pre><code>pprint.pprint(meta['axes'])\naxis_order = ''.join(item['name'] for item in meta['axes'])\nprint(f\"Axis order is {axis_order}\")\n</code></pre> Print the voxel scaling for each resolution level <pre><code>for idx, transform in enumerate(meta['datasets']):\n    print(f\"\\033[1mVoxel transform for the level {idx}:\\033[0m\")\n    pprint.pprint(transform)\n</code></pre></p>"},{"location":"open_local_ome_zarr_zarr-python/#inspect-individual-resolution-layers","title":"Inspect individual resolution layers","text":"<p>Get the top resolution array: <pre><code>zarr_array0 = dataset[0]\nprint(f\"Array type: {type(zarr_array0)}\")\nprint(f\"Shape of the top-level array: {zarr_array0.shape}\")\n</code></pre> Get a downscaled array: <pre><code>zarr_array1 = dataset[1]\nprint(f\"Array type: {type(zarr_array1)}\")\nprint(f\"Shape of the first-level downscaled array: {zarr_array1.shape}\")\n</code></pre> Summarize array-level metadata: <pre><code>zarr_array0.info\nzarr_array1.info\n</code></pre> Print chunk size for the top layer: <pre><code>print(f\"Chunk size: {zarr_array0.chunks}\")\n</code></pre></p> <p>Convert the zarr array to a numpy array: <pre><code>numpy_array0 = zarr_array0[:]\nprint(f\"Array type: {type(numpy_array0)}\")\n# or use numpy directly\nnumpy_array0 = np.array(zarr_array0)\nprint(f\"Array type: {type(numpy_array0)}\")\n</code></pre></p>"},{"location":"open_ome_zarr_ome-zarr-py/","title":"ome-zarr-py","text":"<p>Activate the conda environment <code>ngff_workshop</code>:</p> <pre><code>conda activate ngff_workshop\n</code></pre> <p>Browse into the example data directory and then access Python: </p> <pre><code>cd /path/to/example_omezarrs\npython\n</code></pre>"},{"location":"open_ome_zarr_ome-zarr-py/#import-the-relevant-tools","title":"Import the relevant tools","text":"<pre><code>import ome_zarr, zarr, pprint, os\nfrom ome_zarr.reader import Reader\nfrom ome_zarr.io import parse_url\n</code></pre>"},{"location":"open_ome_zarr_ome-zarr-py/#read-local-and-remote-ome-zarrs","title":"Read local and remote OME-Zarrs","text":"<pre><code># local_path = \"./6001240.zarr\"\nremote_path = \"https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\"\nreader = Reader(parse_url(remote_path))\n# Note here that 'parse_url' can parse both remote and local urls.\n</code></pre> <p>Note that ome-zarr-py uses the term 'node' for different zarr groups and reads them in a flat list. </p> <p>Print the node information per resolution level:</p> <pre><code>nodes = list(reader())\nfor idx, node in enumerate(nodes):\n    print(f\"The node at the level {idx} is {node}\")\n</code></pre> <p>Get the data and metadata of the top-level node</p> <pre><code>dataset = nodes[0].data\nmeta = nodes[0].metadata\n</code></pre> <p>Check the 'data' instance to examine the array shape and the chunks for each resolution layer</p> <pre><code>for idx, array in enumerate(dataset):\n    print(f\"The array {idx} is a {type(array)} and has shape {array.shape} and has chunks with shape {array.chunksize}\")\n</code></pre> <p>Print the axis types and units of the arrays using the metadata instance <pre><code>print(f\"Axis properties of the dataset:\")\npprint.pprint(meta['axes'])\n</code></pre> Print the voxel sizes per resolution level (and any other voxel transforms that may exist) <pre><code>for idx, transforms in enumerate(meta['coordinateTransformations']):\n    print(f\"\\033[1mThe transform metadata for the level {idx}:\\033[0m\")\n    print(f\"{transforms}\")\n</code></pre></p>"},{"location":"open_overview/","title":"Overview","text":"<p>Open local (file system) and remote (S3 object store) OME-Zarr data using various tools</p>"},{"location":"open_overview/#modules","title":"Modules","text":"<ul> <li>n5-ij</li> <li>n5-viewer</li> <li>napari-ome-zarr</li> <li>MoBIE</li> <li>zarr-python</li> <li>ome-zarr-py</li> <li>Web-based Viewers</li> </ul>"},{"location":"save_ome_zarr_fiji/","title":"n5-ij","text":"<p>First open an image on Fiji by dragging and dropping the czi image from your local path (eg., <code>/path/to/data/pff/xyz__multiple_images.czi</code>) as shown below:   A window titled Bioformats Import Options will open. Then click OK without changing any options. Another window titled Bioformats Series Options will open. This shows that the czi file contains two independent images (or \"series\" in bioformats terminology). Then select one of the images and click OK as shown below:  Now open the n5-ij's saving tool via:    - <code>[ File &gt; Save As &gt; HDF5/N5/Zarr/OME-NGFF ... ]</code> as shown below  This will open a window with saving options (which define the properties of the output OME-Zarr) as shown below: </p>"},{"location":"save_ome_zarr_ome-zarr-py/","title":"ome-zarr-py","text":"<p>Save an array as OME-Zarr using the ome-zarr-py library.</p> <p>Create a directory named <code>saved_omezarrs</code> in your home directory,  and browse into it.</p> <pre><code>cd /path/to/data/saved_omezarrs\n</code></pre> <p>Access Python: <pre><code>python\n</code></pre></p>"},{"location":"save_ome_zarr_ome-zarr-py/#do-the-relevant-imports","title":"Do the relevant imports","text":"<pre><code>import zarr, os\nimport numcodecs\nfrom ome_zarr import writer, scale\nfrom ome_zarr.io import parse_url\nfrom skimage.data import astronaut\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#create-fake-data","title":"Create fake data","text":"<pre><code>data = astronaut().swapaxes(0, 2).swapaxes(1, 2)\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#specify-a-scaler","title":"Specify a scaler","text":"<p>In order to create an image pyramid, one has to instantiate a scaler.  This scaler requires the parameters: scale factor, number of resolution layers and downscaling method. <pre><code>scaler = scale.Scaler(downscale=2, # Downscaling factor for x and y axes\n                      max_layer=4, # Number of downscalings = 5\n                      method = 'nearest' # downscaling method\n                      )\n</code></pre></p>"},{"location":"save_ome_zarr_ome-zarr-py/#specify-the-axis-identities-and-the-corresponding-units","title":"Specify the axis identities and the corresponding units","text":"<p>This dictionary will impose the axis order and the units corresponding to  each axis.</p> <pre><code>axes = [\n    dict(name = 'c', type = 'channel'),\n    dict(name = 'y', type = 'space', unit = 'micrometer'),\n    dict(name = 'x', type = 'space', unit = 'micrometer'),\n]\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#specify-the-voxel-sizes-for-each-resolution-level","title":"Specify the voxel sizes for each resolution level","text":"<p>This is a list of list, where the length of the outer list must match  the number of resolution levels. The inner lists contain dictionaries  for different types of coordinate transforms. Each inner list must  contain a scaling transform, a dictionary that takes <code>scale</code> as key  and an iterable of voxel sizes as value.</p> <pre><code>coordinate_transforms = [\n    [{'scale': [1, 0.2, 0.2], 'type': 'scale'}],\n    [{'scale': [1, 0.4, 0.4], 'type': 'scale'}],\n    [{'scale': [1, 0.8, 0.8], 'type': 'scale'}],\n    [{'scale': [1, 1.6, 1.6], 'type': 'scale'}],\n    [{'scale': [1, 3.2, 3.2], 'type': 'scale'}]\n]\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#create-a-zarr-store-to-write","title":"Create a zarr store to write","text":"<p>For the sake of simplicity, here we demonstrate how to write to a local store. It is also possible to write to a remote location by simply specifying a remote  url as input to the <code>parse_url</code> function.</p> <pre><code># Specify the path where you want to write\noutput_path = \"./saved_omezarrs/astronaut.zarr\"\n# Parse the url as a zarr store. Note that \"mode = 'w'\" enables writing to this store.\nstore = parse_url(output_path, mode = 'w').store \nroot = zarr.open_group(store)\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#specify-zarr-storage-options","title":"Specify zarr storage options","text":"<p>The most important zarr storage options are the <code>chunks</code> and the <code>compression</code>  parameters. The <code>chunks</code> parameter is simply a tuple of integers corresponding  to each axis. The <code>compression</code> parameter requires compressor object from  the <code>Numcodecs</code> package, which is a dependency of <code>zarr-python</code>.</p> <pre><code>storage_options=dict(\n                    chunks=(1, 64, 64),  # Output chunk shape\n                    compression = numcodecs.Zlib(), # Compressor to be used, defaults to numcodecs.Blosc()\n                    overwrite = True # Overwrite the output path\n                )\n</code></pre>"},{"location":"save_ome_zarr_ome-zarr-py/#save-the-array","title":"Save the array","text":"<p>Here we use the <code>ome_zarr.writer.write_image</code> function to save the array.  This function takes the parameters specified above as input, downscales the  array accordingly and writes the resulting pyramid to the specified zarr group. </p> <pre><code>writer.write_image(image = data, # In this case, a numpy array\n                   group = root,\n                   axes = axes, # Dimensionality order\n                   scaler=scaler,\n                   coordinate_transformations = coordinate_transforms,\n                   storage_options = storage_options\n                   )\n</code></pre>"},{"location":"save_overview/","title":"Overview","text":"<p>Save data from memory to OME-Zarr using different tools.</p>"},{"location":"save_overview/#modules","title":"Modules","text":"<ul> <li>n5-ij</li> <li>ome-zarr-py</li> </ul>"},{"location":"update_rendering_metadata/","title":"Update rendering metadata","text":"<p>Import the necessary modules</p> <pre><code>import zarr, os\nfrom ome_zarr.io import parse_url, utils\nimport matplotlib.colors as mcolors\n</code></pre> <p>At this stage inspect the image using the OME-Zarr validator:</p> <pre><code># path = f\"{os.path.expanduser('~')}/image_data_formats/day2/astronaut.zarr\"\npath = \"/path/to/astronaut.zarr\"\nutils.view(path) \n</code></pre> <p>Define a utility function to get the hex color code by simple color names</p> <pre><code>def get_color_code(color_name):\n    try:\n        color_code = mcolors.CSS4_COLORS[color_name.lower()]\n        return color_code\n    except KeyError:\n        return f\"Color '{color_name}' not found.\"\n</code></pre> <p>Now add rendering metadata</p> <pre><code>store = parse_url(path, mode = 'w').store # Create a zarr store to save the data. Note that this can also be an s3 object store.\nroot = zarr.open_group(store=store)\nroot.attrs[\"omero\"] = {\n    \"channels\": [\n        {\n            \"color\": get_color_code('cyan'),\n            \"window\": {\"start\": 0, \"end\": 255, \"min\": 0, \"max\": 255},\n            \"label\": \"ch0\",\n            \"active\": True,\n        },\n        {\n            \"color\": get_color_code('magenta'),\n            \"window\": {\"start\": 0, \"end\": 255, \"min\": 0, \"max\": 255},\n            \"label\": \"ch1\",\n            \"active\": True,\n        },\n        {\n            \"color\": get_color_code('yellow'),\n            \"window\": {\"start\": 0, \"end\": 255, \"min\": 0, \"max\": 255},\n            \"label\": \"ch2\",\n            \"active\": True,\n        },\n    ]\n}\n</code></pre> <p>It is important to know here that not all OME-Zarr readers recognize each of these settings. \\ Apply the validator again to the data to see the changes: <pre><code>path = \"/path/to/astronaut.zarr\"\nutils.view(path) \n</code></pre> As the data looks valid, now visualize using different viewers to see if the rendering is working.</p>"},{"location":"validate_ome_zarr_ome-zarr-validator/","title":"OME-Zarr Validator","text":"<p>Validate local and remote OME-Zarrs using the OME-Zarr validator.</p>"},{"location":"validate_ome_zarr_ome-zarr-validator/#validate-local-data","title":"Validate local data","text":""},{"location":"validate_ome_zarr_ome-zarr-validator/#terminal","title":"Terminal","text":"<p>Activate the conda environment <code>ngff_workshop</code>:</p> <pre><code>conda activate ngff_workshop\n</code></pre> <p>Browse into the example data directory </p> <pre><code>cd /path/to/example_omezarrs\n</code></pre> <p>Validate one of the example OME-Zarrs:</p> <pre><code>ome_zarr view /path/to/example_omezarrs/6001240.zarr\n</code></pre> <p>The validator will open in a web browser and demonstrate various metadata fields of the OME-Zarr dataset.</p> <ul> <li>Find out the metadata fields such as axes, units and scales.</li> <li>Check the array and chunk shapes and bytes per resolution level.</li> <li>Visualize a single chunk.</li> </ul>"},{"location":"validate_ome_zarr_ome-zarr-validator/#python","title":"Python","text":"<p>Make sure the environment <code>ngff_workshop</code> is active and you are in the <code>example_omezarrs</code> folder.</p> <p>Then access Python:</p> <pre><code>python\n</code></pre> <p>Do the relevant imports and validate the dataset: <pre><code>from ome_zarr import utils\nutils.view(input_path = \"./6001240.zarr\")\n</code></pre></p>"},{"location":"validate_ome_zarr_ome-zarr-validator/#validate-remote-data","title":"Validate remote data","text":"<p>Now the aim is to validate the remotely stored version of the same dataset.</p> <p>Enter the following into your browser: </p> <pre><code>https://ome.github.io/ome-ngff-validator/?source=\n</code></pre> <p>Then paste the following dataset url after the 'equal' sign: </p> <pre><code>https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr\n</code></pre> <p>Thus construct the following link: </p> <p>https://ome.github.io/ome-ngff-validator/?source=https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0062A/6001240.zarr </p> <p>Note that with the remote url it is possible to copy the link from your browser and share it with  your colleagues.</p>"},{"location":"web_based_viewers/","title":"Web-based viewers","text":""},{"location":"web_based_viewers/#vizarr-2d-channels","title":"Vizarr (2D + Channels)","text":"<p>Create a link by pasting the address of the dataset after the <code>=</code> in the following link:</p> <pre><code>https://hms-dbmi.github.io/vizarr/?source=\n</code></pre> <p>For example, paste the following address: <pre><code>https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0\n</code></pre></p> <p>Thus construct the following link: https://hms-dbmi.github.io/vizarr/?source=https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0</p> <p>Click on it or paste it in the browser to open the viewer.</p>"},{"location":"web_based_viewers/#itk-vtk-viewer-3d-channels","title":"itk-vtk-viewer (3D + channels)","text":"<p>Create a link by pasting the address of the dataset after the <code>=</code> <pre><code>https://kitware.github.io/itk-vtk-viewer/app/?fileToLoad=\n</code></pre></p> <p>For example, paste the following address: <pre><code>https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0\n</code></pre></p> <p>Thus construct the following link: https://kitware.github.io/itk-vtk-viewer/app/?fileToLoad=https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0</p> <p>Click on it or paste it in the browser to open the viewer.</p>"},{"location":"web_based_viewers/#allen-3d-volume-viewer-4d-channels","title":"Allen 3D Volume Viewer (4D + channels)","text":"<p>Create a link by pasting the address of the dataset after the <code>=</code> <pre><code>https://volumeviewer.allencell.org/viewer?url=\n</code></pre></p> <p>For example, paste the following address: <pre><code>https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0\n</code></pre></p> <p>Thus create the following link: https://volumeviewer.allencell.org/viewer?url=https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/S-BSST410/IM2/IM2.zarr/0</p> <p>Click on it or paste it in the browser to open the viewer.</p>"},{"location":"web_based_viewers/#neuroglancer-2d-channels-volume-rendering-experimental","title":"Neuroglancer (2D + channels, volume rendering experimental)","text":"<ul> <li>Go to https://neuroglancer-demo.appspot.com/</li> <li>On to top right in <code>Source</code>, paste the following:  <pre><code>zarr://https://s3.embl.de/i2k-2020/platy-raw.ome.zarr\n</code></pre></li> <li>Press Enter (multiple times). </li> <li>Navigate around in the sample </li> <li>Zoom in/out: <code>ctrl + mouse scroll</code></li> <li>Neuroglancer enables sharing of views. The URL in your browser adapts to your current view. Simply copy and paste the URL to share a view with a collaborator</li> </ul>"},{"location":"web_based_viewers/#bioimage-archive","title":"BioImage Archive","text":"<p>The BioImage Archive also provides visualisations for the datasets they store, using the tools discussed above.  Click on the link below to see an example. https://uk1s3.embassy.ebi.ac.uk/bia-integrator-data/pages/S-BSST410/IM2.html An advantage of the BioImage Archive is that they also provide a range of image metadata along with the visualisation.</p>"},{"location":"what_is_ngff/","title":"Introduction to OME-Zarr","text":"<p>OME-Zarr is a chunked file format represented in nested directories and supporting multiple resolution layers. </p> <p>This structure offers faster access to data that is remotely stored. It also allows faster parallel writing to remote storage.</p>"},{"location":"what_is_ngff/#what-would-the-simplest-ome-zarr-look-like","title":"What would the simplest OME-Zarr look like?","text":"<p>Folder structure of a simple OME-Zarr with one chunk and  one resolution layer:</p> \ud83d\udcc2 filament-single-chunk.zarr (Zarr group with .zattrs (multiscales) metadata)  \u2514\u2500\u2500 \ud83d\udcc1 0 (Resolution level index, Zarr array with .zarray (storage) metadata)  \u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Time chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Channel chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Z chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Y chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (X chunk index) <p>Content of the .zattrs metadata file: <pre><code>{\n    \"multiscales\": [\n        {\n            \"name\": \"filament-single-chunk.zarr\",\n            \"version\": \"0.4\",\n            \"axes\": [\n                { \"name\": \"t\", \"type\": \"time\", \"unit\": \"s\" },\n                { \"name\": \"c\", \"type\": \"channel\", \"unit\": \"Channel\" },\n                { \"name\": \"z\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"y\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"x\", \"type\": \"space\", \"unit\": \"\u03bcm\" }\n            ],\n            \"datasets\": [\n                {\n                    \"path\": \"0\",\n                    \"coordinateTransformations\": [\n                        {\n                            \"type\": \"scale\",\n                            \"scale\": [1.0, 1, 0.23985, 0.021462, 0.021462]\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Content of the .zarray metadata file:</p> <pre><code>{\n    \"chunks\": [1, 1, 29, 253, 246],\n    \"shape\": [1, 1, 29, 253, 246],\n    \"dtype\": \"|u1\",\n    \"order\": \"C\",\n    \"dimension_separator\": \"/\",\n    \"fill_value\": 0,\n    \"filters\": null,\n    \"compressor\": {\n        \"id\": \"blosc\",\n        \"cname\": \"lz4\",\n        \"clevel\": 5,\n        \"shuffle\": 1,\n        \"blocksize\": 0\n    },\n    \"zarr_format\": 2\n}\n</code></pre>"},{"location":"what_is_ngff/#what-if-it-had-multiple-chunks","title":"What if it had multiple chunks?","text":"\ud83d\udcc2 filament-three-z-chunks.zarr(Zarr group with .zattrs (multiscales) metadata) \u2514\u2500\u2500 \ud83d\udcc1 0 (Resolution level index, Zarr array with .zarray (storage) metadata)  \u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Time chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Channel chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc1 0 (z chunk index 0)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2502\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (y chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (x chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u251c\u2500\u2500 \ud83d\udcc1 1 (z chunk index 1)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2502\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (y chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (x chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 2 (z chunk index 2)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (y chunk index)  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (x chunk index) <p>Content of the .zattrs metadata file:</p> <pre><code>{\n    \"multiscales\": [\n        {\n            \"name\": \"filament-single-chunk.zarr\",\n            \"version\": \"0.4\",\n            \"axes\": [\n                { \"name\": \"t\", \"type\": \"time\", \"unit\": \"s\" },\n                { \"name\": \"c\", \"type\": \"channel\", \"unit\": \"Channel\" },\n                { \"name\": \"z\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"y\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"x\", \"type\": \"space\", \"unit\": \"\u03bcm\" }\n            ],\n            \"datasets\": [\n                {\n                    \"path\": \"0\",\n                    \"coordinateTransformations\": [\n                        {\n                            \"type\": \"scale\",\n                            \"scale\": [1.0, 1, 0.23985, 0.021462, 0.021462]\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <p>Content of the .zarray metadata file:</p> <pre><code>{\n    \"chunks\": [1, 1, 10, 253, 246],\n    \"shape\": [1, 1, 29, 253, 246],\n    \"dtype\": \"|u1\",\n    \"order\": \"C\",\n    \"dimension_separator\": \"/\",\n    \"fill_value\": 0,\n    \"filters\": null,\n    \"compressor\": {\n        \"id\": \"blosc\",\n        \"cname\": \"lz4\",\n        \"clevel\": 5,\n        \"shuffle\": 1,\n        \"blocksize\": 0\n    },\n    \"zarr_format\": 2\n}\n</code></pre>"},{"location":"what_is_ngff/#what-if-it-had-multiple-resolution-levels","title":"What if it had multiple resolution levels?","text":"\ud83d\udcc2 filament-two-layers-single-chunk.zarr    \u251c\u2500\u2500 \ud83d\udcc1 0 (Resolution level 0, Zarr array with  .zarray (storage) metadata))    \u2502\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Time chunk index)    \u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Channel chunk index)    \u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (z chunk index)    \u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (y chunk index)    \u2502\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (x chunk index)    \u2514\u2500\u2500 \ud83d\udcc1 1 (Resolution level 1, Zarr array with  .zarray (storage) metadata))    \u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Time chunk index)    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (Channel chunk index)    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (z chunk index)    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (y chunk index)    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2514\u2500\u2500 \ud83d\udcc1 0 (x chunk index) <p>Content of the .zattrs metadata file: <pre><code>{\n    \"multiscales\": [\n        {\n            \"name\": \"filament.zarr\",\n            \"version\": \"0.4\",\n            \"axes\": [\n                { \"name\": \"t\", \"type\": \"time\", \"unit\": \"s\" },\n                { \"name\": \"c\", \"type\": \"channel\", \"unit\": \"Channel\" },\n                { \"name\": \"z\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"y\", \"type\": \"space\", \"unit\": \"\u03bcm\" },\n                { \"name\": \"x\", \"type\": \"space\", \"unit\": \"\u03bcm\" }\n            ],\n            \"datasets\": [\n                {\n                    \"path\": \"0\",\n                    \"coordinateTransformations\": [\n                        { \n                            \"type\": \"scale\", \n                            \"scale\": [1.0, 1, 0.23985, 0.021462000487230338, 0.021462000487230338] \n                        }\n                    ]\n                },\n                {\n                    \"path\": \"1\",\n                    \"coordinateTransformations\": [\n                        { \n                            \"type\": \"scale\", \n                            \"scale\": [1.0, 1.0, 0.49683214285714294, 0.04309433431166092, 0.042924000974460676] \n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Content of the .zarray metadata files corresponding to levels 0 and 1:</p> <pre><code>{\n    \"chunks\": [1, 1, 29, 253, 246],\n    \"shape\": [1, 1, 29, 253, 246],\n    \"dtype\": \"|u1\",\n    \"order\": \"C\",\n    \"dimension_separator\": \"/\",\n    \"fill_value\": 0,\n    \"filters\": null,\n    \"compressor\": {\n        \"id\": \"blosc\",\n        \"cname\": \"lz4\",\n        \"clevel\": 5,\n        \"shuffle\": 1,\n        \"blocksize\": 0\n    },\n    \"zarr_format\": 2\n}\n</code></pre> <pre><code>{\n    \"chunks\": [1, 1, 15, 127, 123],\n    \"shape\": [1, 1, 15, 127, 123],\n    \"dtype\": \"|u1\",\n    \"order\": \"C\",\n    \"dimension_separator\": \"/\",\n    \"fill_value\": 0,\n    \"filters\": null,\n    \"compressor\": {\n        \"id\": \"blosc\",\n        \"cname\": \"lz4\",\n        \"clevel\": 5,\n        \"shuffle\": 1,\n        \"blocksize\": 0\n    },\n    \"zarr_format\": 2\n}\n</code></pre>"}]}